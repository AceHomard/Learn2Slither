# ğŸ Learn2Slither - Reinforcement Learning Snake Game

## ğŸ“– Overview

**Learn2Slither** is a reinforcement learning project where an AI agent learns to play the classic Snake game using **Q-Learning**. The agent interacts with the environment, receives rewards, and optimizes its policy to maximize performance over time.

---

## âœ¨ Features

âœ… **Q-Learning Implementation** with a reduced state space (256 states).  
âœ… **Customizable Grid Size** via command-line arguments.  
âœ… **Graphical and Terminal Visualization** options.  
âœ… **Training and Evaluation Modes** (with or without learning).  
âœ… **Model Saving/Loading** for training persistence.  
âœ… **Step-by-Step Mode** for live evaluation of the agent's decisions.  

---

## ğŸ“¥ Installation

### Clone the Repository
    
```bash
git clone https://github.com/AceHomard/Learn2Slither.git
cd Learn2Slither
```
    
### Create a Virtual Environment (Recommended)
```bash
python -m venv venv  # Create a virtual environment
source venv/bin/activate  # On macOS/Linux
venv\Scripts\activate  # On Windows
```
### Install Dependencies
```bash
pip install -r requirements.txt 
```
---

## ğŸš€ Usage
Train the Snake

Run the script with:
```bash
python main.py -visual on
```
---

## âš ï¸ Important Notice

If `-visual on` is enabled, the **step-by-step mode** is **active by default**.  
- Press **`O`** to **disable** step-by-step mode.  
- Press **`P`** to **re-enable** it.  

In step-by-step mode, you must **press the `Space` bar** to advance each step of the snake's movement.

---
## ğŸ“Œ Available Arguments

| Argument      | Description                                      | Default |
|--------------|--------------------------------------------------|---------|
| `-visual`    | Enable (`on`) or disable (`off`) the graphical display | `on`    |
| `-load`      | Load a pre-trained model (`.npy` file)           | `None`  |
| `-sessions`  | Number of training episodes                     | `250`   |
| `-dontlearn` | Disable learning (evaluation mode)              | `False` |
| `-dontsave`  | Prevent model saving                            | `False` |
| `-noepsil`   | Disable exploration (always exploit best action) | `off`   |
| `-displayterm` | Show the state matrix in terminal             | `off`   |
| `-grid`      | Set the grid size                               | `10`    |

---
## ğŸ¯ Example Commands

1ï¸âƒ£ Train for 1000 episodes (fast training without visualization)
```bash
python main.py -visual off -sessions 1000
```
2ï¸âƒ£ Load a pre-trained model and evaluate
```bash
python main.py -sessions 10 -load models/1000sess.npy -dontlearn -noepsil on
```
---

## ğŸ›  How It Works

The Snake's environment is simplified into a **4^4 = 256 state space**.

The agent "sees" its surroundings as an encoded vector of values.
## Q-Learning Algorithm
- Exploration vs Exploitation: Uses epsilon-greedy to balance randomness and optimal moves.

- Q-Table Update: The agent updates its Q-table based on rewards:
    > Q(s,a)â†Q(s,a)+Î±[r+Î³maxâ¡Q(sâ€²,aâ€²)âˆ’Q(s,a)]
---

## ğŸ¯ Rewards System

| Action                     | Reward  |
|----------------------------|---------|
| Eating a green apple       | `+50`   |
| Eating a red apple         | `-10`   |
| Hitting a wall or itself   | `-50`   |
| Normal move                | `-0.1`  |

---

## ğŸ’¾ Model Saving & Loading

The model is stored as a .npy file in the models/ directory.

    Save after training: models/{sessions}sess.npy
    Load an existing model: Use -load with the filename.
---
## ğŸ“Š Results Visualization

The script generates graphs to analyze learning progress:

ğŸ“ˆ Snake length evolution over episodes.

ğŸ“‰ Number of moves per episode.

## ğŸ–¼ï¸ Screenshots

### Training Mode (Graphical View)
![Training Mode](snake_1000sess.png)
